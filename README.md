# Automatic Differentiation (AutoGrad)

A **custom implementation** of the **automatic differentiation** mechanism in **Python** reflecting usage patterns found in **common deep learning frameworks**. It is a **simplified** version that primarily demonstrates the implementation of **automatic differentation** and **backpropagation mechanism**.

The project structure has been partially **inspired** by the [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) libraries.

## Preliminaries

Here is a list of **necessary preliminaries** both for the **general Python programming** as well as **autograd**-related **concepts**.

### Python-related foundations (domain-agnostic)

* [Closures](https://zetcode.com/python/python-closures/) - core Python **concept** - **blog** post.
* [Type hints](https://docs.python.org/3/library/typing.html) - standard Python **extension** - official **documentation**.
* [Generators](https://wiki.python.org/moin/Generators) - core Python **concept** - **wiki** page.
* [Dataclasses](https://docs.python.org/3/library/dataclasses.html) - useful Python **module** - official **documentation**.

### AutoGrad-related foundations (domain-specific)

* Strong [foundations of NumPy](https://numpy.org/doc/stable/user/basics.html) - official **documentation**.
  * Especially [broadcasting in NumPy](https://numpy.org/doc/stable/user/basics.broadcasting.html) - official **documentation**.
* [Vectorization and broadcasting in NumPy](https://blog.paperspace.com/numpy-optimization-vectorization-and-broadcasting/) - **blog** post.
* [Automatic differentiation in PyTorch](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html) - official **documentation**.

## Credits

* Author: ***Milan Ondrašovič***.
* License: **MIT**.